{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tworzymy środowisko wirtualne i instalujemy wymagane pakiety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m venv .env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pamiętaj o wybraniu interpretera ze środowiska .env (w przypadku VS Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source .env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install gymnasium\n",
    "!pip install 'gymnasium[box2d]'\n",
    "!pip install torch\n",
    "!pip install 'numpy<2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ogólne przedstawienie problemu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "terminated = False\n",
    "truncated = False\n",
    "\n",
    "while not (terminated or truncated):\n",
    "    action = 1\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    env.render()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Co zawiera stan zwracany przez środowisko?\n",
    "\n",
    "- **pozycja pozioma** lądownika (x)\n",
    "- **pozycja pionowa** lądownika (y)\n",
    "- **prędkość pozioma** lądownika\n",
    "- **prędkość pionowa** lądownika\n",
    "- **kąt nachylenia** lądownika\n",
    "- **prędkość kątowa** lądownika\n",
    "- **czy noga nr 1 (lewa) lądownika ma styczność z podłożem**\n",
    "- **czy noga nr 2 (prawa) lądownika ma styczność z podłożem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ogólne dane dotyczące samego środowiska i możliwych akcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ilość możliwych akcji: 4\n"
     ]
    }
   ],
   "source": [
    "print(f'Ilość możliwych akcji: {env.action_space.n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jakie akcje są możliwe?\n",
    "- **Akcja 0**: Brak działania\n",
    "- **Akcja 1**: Uruchom główny silnik\n",
    "- **Akcja 2**: Uruchom lewy boczny silnik\n",
    "- **Akcja 3**: Uruchom prawy boczny silnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ustalamy hardware pod obliczenia (CPU/GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected device: CPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "OPTIMIZE_WITH_HARDWARE = False\n",
    "\n",
    "device = torch.device('cpu')\n",
    "if OPTIMIZE_WITH_HARDWARE:\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device('mps')\n",
    "        print(f'Selected device: MPS (Metal Performance Shaders)')\n",
    "    elif torch.backends.cuda.is_available():\n",
    "        device = torch.device('cuda')\n",
    "        print(f'Selected device: GPU with CUDA support')\n",
    "else:\n",
    "    print(f'Selected device: CPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiujemy strukturę naszej sieci głębokiej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class DQNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, action_size)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = torch.nn.functional.leaky_relu(self.fc1(state), negative_slope=0.01)\n",
    "        x = torch.nn.functional.leaky_relu(self.fc2(x), negative_slope=0.01)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiujemy obiekt agenta sieci DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from torch import optim\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Hiperparametry treningu sieci DQN\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "class DQNAgent(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.state_size = state_size        # ilość informacji dot. stanu środowiska\n",
    "        self.action_size = action_size      # ilość akcji, które agent może wykonać\n",
    "        self.discount_factor = 0.99         # współczynnik spadku wartości nagrody\n",
    "        self.epsilon_greedy = 1.0           # początkowy współczynnik losowości (1 = 100% losowości)\n",
    "        self.epsilon_greedy_min = 0.1       # minimalny współczynnik losowości\n",
    "        self.epsilon_greedy_decay = 0.995   # zmniejszanie stopnia losowości co iterację o 5%\n",
    "        self.memory = deque(maxlen=1000)    # kolekcja przechowująca 1000 ostatnich zdarzeń\n",
    "        self.train_start = 500              # liczba zdarzeń, od której zaczynamy trenować model\n",
    "\n",
    "        self.model = DQNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LEARNING_RATE)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    # Zapisuje podjętą akcję w danym stanie i jej skutki \n",
    "    def memorize(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    # Wybiera akcje dla danego stanu. Jeśli aktualnie model\n",
    "    # nie eksploruje (wykonuje losową akcje) to wybierana jest\n",
    "    # akcja o najlepszym potencjale (najwyższa wartość nagrody)\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon_greedy:\n",
    "            return random.randrange(self.action_size)\n",
    "        # unsqueeze zapewnia odpowiedni wymiar [batch_size, state_size]\n",
    "        # PyTorch narzuca format danych treningowych w postaci tensora, który\n",
    "        # w pierwszym wymiarze zawiera informację i ilości paczek a następnie same\n",
    "        # dane treningowe, dlatego 'unsqueeze' rozszerza wymiar danych mimo tego, że\n",
    "        # mamy tylko jedną paczkę w tej funkcji\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values_predicted = self.model(state)\n",
    "        return torch.argmax(q_values_predicted).item()\n",
    "    \n",
    "\n",
    "    def replay(self):\n",
    "        # Nie zaczynamy trenować modelu dopóki nie zbierzemy\n",
    "        # minimalnej ilości danych w buforze memory\n",
    "        if len(self.memory) < self.train_start:\n",
    "            return\n",
    "        \n",
    "        data_batch = random.sample(self.memory, BATCH_SIZE) # Losujemy paczkę danych do treningu\n",
    "        \n",
    "        total_mse_loss = 0\n",
    "        for state, action, reward, next_state, done in data_batch:\n",
    "            state = torch.FloatTensor(state)\n",
    "            next_state = torch.FloatTensor(next_state)\n",
    "            reward = torch.FloatTensor([reward])\n",
    "            discounted_reward = reward\n",
    "            if not done:\n",
    "                discounted_reward += self.discount_factor * torch.max(self.model(next_state))\n",
    "            \n",
    "            dqn_prediction = self.model(state)\n",
    "            true_reward = dqn_prediction.clone()     # Tworzymy klon aby nadpisać wynik dla akcji niżej\n",
    "            true_reward[action] = discounted_reward  # Nadpisujemy wartość nagrody dla wykonanej akcji\n",
    "            \n",
    "            loss = self.criterion(dqn_prediction, true_reward)\n",
    "            \n",
    "            self.optimizer.zero_grad()  # Zerujemy gradient\n",
    "            loss.backward()             # Liczymy gradient\n",
    "            self.optimizer.step()       # Aktualizujemy wagi sieci\n",
    "\n",
    "            total_mse_loss += loss.item()\n",
    "        \n",
    "        # Jeśli nie doszliśmy do minimalnej wartości współczynnika\n",
    "        # eksploracji to nadal go zmniejszamy z każdą iteracją\n",
    "        if self.epsilon_greedy > self.epsilon_greedy_min:\n",
    "            self.epsilon_greedy *= self.epsilon_greedy_decay\n",
    "        \n",
    "        return total_mse_loss / BATCH_SIZE # zwracamy średni błąd MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funkcja do zapisu oraz odtworzenia naszego modelu, aby nie stracić danych podczas treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, episode, filename=\"model_checkpoint.pth\"):\n",
    "    checkpoint = {\n",
    "        'episode': episode,  # Numer epoki, aby można było wznowić trening\n",
    "        'model_state_dict': model.state_dict(),  # Wagi modelu\n",
    "        'optimizer_state_dict': optimizer.state_dict()  # Parametry optymalizatora\n",
    "    }\n",
    "    torch.save(checkpoint, filename)  # Zapisujemy do pliku\n",
    "\n",
    "def load_checkpoint(model, optimizer, filename):\n",
    "    checkpoint = torch.load(filename)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])  # Załaduj wagi modelu\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])  # Załaduj stan optymalizatora\n",
    "    episode = checkpoint['episode']  # Załaduj numer epoki\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definiujemy pętlę treningową modelu DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "\n",
    "env = gym.make(\"LunarLander-v2\", render_mode=\"human\")\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Kolekcje do śledzenia postępów treningu\n",
    "rewards_history = []\n",
    "epsilon_history = []\n",
    "loss_history = []\n",
    "\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state, _ = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    total_reward = 0\n",
    "    total_mse_loss = 0\n",
    "    step_counter = 0\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        \n",
    "        agent.memorize(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        step_counter += 1\n",
    "\n",
    "        mse_loss = agent.replay()\n",
    "        if mse_loss is not None:\n",
    "            total_mse_loss += mse_loss\n",
    "        \n",
    "        if done:\n",
    "            average_loss = total_mse_loss / step_counter if step_counter > 0 else 0\n",
    "            print(f\"Episode: {episode+1}/{episodes}, Reward: {total_reward:.2f}, \n",
    "                  Epsilon: {agent.epsilon_greedy:.3f}, MSE: {total_mse_loss:.3f} \n",
    "                  Loss: {math.sqrt(total_mse_loss):.2f}\")\n",
    "\n",
    "            # Zapisujemy dane opisujące postępy treningu\n",
    "            rewards_history.append(total_reward)\n",
    "            epsilon_history.append(agent.epsilon_greedy)\n",
    "            loss_history.append(math.sqrt(average_loss))\n",
    "            break\n",
    "    \n",
    "    # Zapisujemy nasz trenowany model co 10 epizod\n",
    "    if episode % 10 == 0:\n",
    "        save_checkpoint(agent.model, agent.optimizer, \n",
    "                        episode, filename=f\"model_checkpoint_{episode}.pth\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
